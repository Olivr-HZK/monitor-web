#!/usr/bin/env python3
"""
æŒ‰å‰ç«¯ã€Œä¼‘é—²æ¸¸æˆæ£€æµ‹ã€ä¸­ä¸¤ä¸ªå°æ¸¸æˆå‘¨æŠ¥çš„æ ¼å¼æ„å»ºå†…å®¹ï¼Œå¹¶å‘é€åˆ°é£ä¹¦å’Œä¼ä¸šå¾®ä¿¡ï¼š
  1. å¾®ä¿¡/æŠ–éŸ³å°æ¸¸æˆå‘¨æŠ¥ï¼ˆæ¥è‡ª public/videos.db çš„ weekly_report_simpleï¼‰
  2. SensorTower å‘¨æŠ¥ï¼ˆæ¥è‡ª public/sensortower_top100.db çš„ rank_changesï¼‰

é£ä¹¦ï¼šå‘ä¸€æ¡äº’åŠ¨å¡ç‰‡ï¼ˆinteractive cardï¼Œå†…å®¹ä¸º Markdownï¼‰ã€‚
ä¼ä¸šå¾®ä¿¡ï¼šå‘ä¸€æ¡ Markdown æ¶ˆæ¯ã€‚

ç¯å¢ƒå˜é‡ï¼ˆ.env æˆ–ç³»ç»Ÿç¯å¢ƒï¼‰ï¼š
  - FEISHU_WEBHOOK_URLï¼šé£ä¹¦è‡ªå®šä¹‰æœºå™¨äºº Webhook
  - WECOM_WEBHOOK_URL_REALï¼šä¼ä¸šå¾®ä¿¡è‡ªå®šä¹‰æœºå™¨äºº Webhook

ä½¿ç”¨æ–¹å¼ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼š
  python scripts/send_minigame_weekly_reports.py
  python scripts/send_minigame_weekly_reports.py --videos-db public/videos.db --sensortower-db public/sensortower_top100.db
"""

import argparse
import json
import os
import re
import sqlite3
import sys
import urllib.error
import urllib.request
from pathlib import Path

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None


def _load_env(repo_root: Path) -> None:
    """ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env åˆ° os.environã€‚æœ‰ dotenv ç”¨ dotenvï¼Œå¦åˆ™ç®€å•è§£æã€‚"""
    env_path = repo_root / ".env"
    if not env_path.exists():
        return
    if load_dotenv is not None:
        load_dotenv(env_path)
        return
    # æ—  python-dotenv æ—¶ç®€å•è§£æ KEY=VALUEï¼ˆå¿½ç•¥ç©ºè¡Œã€# æ³¨é‡Šã€å»å¼•å·ï¼‰
    for line in env_path.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            continue
        key, _, value = line.partition("=")
        key = key.strip()
        value = value.strip()
        if value.startswith('"') and value.endswith('"'):
            value = value[1:-1].strip()
        elif value.startswith("'") and value.endswith("'"):
            value = value[1:-1].strip()
        if key:
            os.environ[key] = value


DETAIL_LINK = "https://olivr-hzk.github.io/monitor-web/"
WEEKLY_BRIEF_PLATFORM = {"wx": "å¾®ä¿¡å°æ¸¸æˆ", "dy": "æŠ–éŸ³å°æ¸¸æˆ"}


# ---------- å¾®ä¿¡/æŠ–éŸ³å°æ¸¸æˆå‘¨æŠ¥ï¼ˆä¸å‰ç«¯ reportsLoader.loadWeeklyBriefFromDb ä¸€è‡´ï¼‰----------
def build_wechat_douyin_weekly_md(conn: sqlite3.Connection) -> str | None:
    """ä» weekly_report_simple å–æœ€æ–°ä¸€å‘¨ï¼Œç”Ÿæˆä¸å‰ç«¯ä¸€è‡´çš„å‘¨æŠ¥ Markdownã€‚"""
    cur = conn.cursor()
    try:
        cur.execute(
            """
            SELECT week_range, platform, game_name, change_type, rank, rank_change
            FROM weekly_report_simple
            WHERE platform IN ('wx', 'dy')
            ORDER BY week_range DESC, platform, change_type, CAST(rank AS INTEGER)
            """
        )
        rows = [
            {
                "week_range": r[0],
                "platform": r[1],
                "game_name": r[2],
                "change_type": r[3],
                "rank": r[4],
                "rank_change": r[5],
            }
            for r in cur.fetchall()
        ]
    except sqlite3.OperationalError as e:
        print(f"[å¾®ä¿¡/æŠ–éŸ³] è¯»å– weekly_report_simple å¤±è´¥: {e}", file=sys.stderr)
        return None

    if not rows:
        return None

    by_week: dict[str, list] = {}
    for r in rows:
        w = r["week_range"] or ""
        if not w:
            continue
        if w not in by_week:
            by_week[w] = []
        by_week[w].append(r)

    # å–æœ€æ–°ä¸€å‘¨
    latest_week = max(by_week.keys())
    week_rows = by_week[latest_week]
    new_in = [r for r in week_rows if r["change_type"] == "æ–°è¿›æ¦œ"]
    surge = [r for r in week_rows if r["change_type"] == "é£™å‡"]

    lines = [
        f"# å‘¨æŠ¥ç®€è¦ {latest_week}",
        "",
        f"**ç›‘æ§æ—¶é—´**ï¼š{latest_week}",
        "",
    ]
    if new_in:
        lines.append("## æœ¬å‘¨æ–°è¿›æ¦œ")
        lines.append("")
        for r in new_in:
            label = WEEKLY_BRIEF_PLATFORM.get(r["platform"], r["platform"])
            lines.append(f"- **{r['game_name']}**ï¼ˆ{label}ï¼‰")
        lines.append("")
    if surge:
        lines.append("## æœ¬å‘¨æ’åé£™å‡")
        lines.append("")
        for r in surge:
            label = WEEKLY_BRIEF_PLATFORM.get(r["platform"], r["platform"])
            lines.append(f"- **{r['game_name']}**ï¼ˆ{label}ï¼Œæ’åå˜åŒ– {r['rank_change']}ï¼‰")
        lines.append("")
    if not new_in and not surge:
        lines.append("è¯¥å‘¨æš‚æ— æ–°è¿›æ¦œæˆ–æ’åé£™å‡è®°å½•ã€‚")
        lines.append("")
    lines.append("---")
    lines.append("")
    lines.append(f"è¯¦ç»†ç©æ³•è¯·ç™»å½• [ç›‘æµ‹æ±‡æ€»å¹³å°]({DETAIL_LINK}) æŸ¥çœ‹ã€‚")
    return "\n".join(lines)


# ---------- SensorTower å‘¨æŠ¥ï¼ˆä¸å‰ç«¯ sensortowerWeeklyReport + generate_sensortower_weekly_report ä¸€è‡´ï¼‰----------
def _parse_surge(change: str) -> int:
    if not change or change == "NEW":
        return 0
    m = re.search(r"â†‘\s*(\d+)", str(change).strip())
    return int(m.group(1)) if m else 0


def _fmt_num(n) -> str:
    if n is None:
        return "â€”"
    try:
        n = int(n)
    except (TypeError, ValueError):
        return str(n)
    if n >= 10000:
        return f"{n / 10000:.2f}ä¸‡"
    return f"{n:,}"


def _fmt_revenue(r) -> str:
    if r is None:
        return "â€”"
    try:
        r = float(r)
    except (TypeError, ValueError):
        return str(r)
    if r >= 10000:
        return f"${r / 10000:.2f}ä¸‡"
    return f"${r:,.0f}"


def build_sensortower_weekly_md(conn: sqlite3.Connection) -> str | None:
    """ä» rank_changes å–æœ€æ–°ä¸€å‘¨ï¼Œç”Ÿæˆä¸å‰ç«¯ä¸€è‡´çš„ SensorTower å‘¨æŠ¥ Markdownã€‚"""
    cur = conn.cursor()
    try:
        cur.execute(
            """
            SELECT DISTINCT rank_date_current, rank_date_last
            FROM rank_changes
            ORDER BY rank_date_current DESC
            LIMIT 1
            """
        )
        row = cur.fetchone()
        if not row:
            return None
        rank_date_current, rank_date_last = row[0], row[1] or ""
    except sqlite3.OperationalError as e:
        print(f"[SensorTower] è¯»å– rank_changes å¤±è´¥: {e}", file=sys.stderr)
        return None

    cur.execute(
        """
        SELECT
            r.current_rank,
            r.last_week_rank,
            r.change,
            r.app_id,
            r.country,
            r.platform,
            r.downloads,
            r.revenue,
            COALESCE(m.name, r.app_name, r.app_id) AS display_name,
            COALESCE(NULLIF(TRIM(r.publisher_name), ''), m.publisher_name) AS publisher_name
        FROM rank_changes r
        LEFT JOIN app_metadata m ON m.app_id = r.app_id AND m.os = LOWER(r.platform)
        WHERE r.rank_date_current = ?
          AND r.change_type = 'ğŸ†• æ–°è¿›æ¦œå•'
          AND r.current_rank <= 50
        ORDER BY r.current_rank ASC, r.country, r.platform
        """,
        (rank_date_current,),
    )
    new_top50 = [
        {
            "current_rank": r[0],
            "last_week_rank": r[1],
            "change": r[2],
            "app_id": r[3],
            "country": r[4],
            "platform": r[5],
            "downloads": r[6],
            "revenue": r[7],
            "display_name": r[8] or r[3],
            "publisher_name": r[9] or "â€”",
        }
        for r in cur.fetchall()
    ]

    cur.execute(
        """
        SELECT
            r.current_rank,
            r.last_week_rank,
            r.change,
            r.app_id,
            r.country,
            r.platform,
            r.downloads,
            r.revenue,
            COALESCE(m.name, r.app_name, r.app_id) AS display_name,
            COALESCE(NULLIF(TRIM(r.publisher_name), ''), m.publisher_name) AS publisher_name
        FROM rank_changes r
        LEFT JOIN app_metadata m ON m.app_id = r.app_id AND m.os = LOWER(r.platform)
        WHERE r.rank_date_current = ?
          AND r.change_type = 'ğŸš€ æ’åé£™å‡'
        ORDER BY r.current_rank ASC
        """,
        (rank_date_current,),
    )
    surge_rows = [
        {
            "current_rank": r[0],
            "last_week_rank": r[1],
            "change": r[2],
            "surge_value": _parse_surge(r[2] or ""),
            "app_id": r[3],
            "country": r[4],
            "platform": r[5],
            "downloads": r[6],
            "revenue": r[7],
            "display_name": r[8] or r[3],
            "publisher_name": r[9] or "â€”",
        }
        for r in cur.fetchall()
    ]
    surge_rows.sort(key=lambda x: (-x["surge_value"], x["current_rank"]))
    surge_top10 = surge_rows[:10]

    lines = [
        f"# SensorTower å‘¨æŠ¥ï¼ˆ{rank_date_current}ï¼‰",
        "",
        f"**ç»Ÿè®¡å‘¨æœŸ**ï¼šæœ¬å‘¨æ¦œå•æ—¥æœŸ {rank_date_current}ï¼Œå¯¹æ¯”ä¸Šå‘¨ {rank_date_last}ã€‚",
        "",
        "---",
        "",
        "## ä¸€ã€æœ¬å‘¨æ–°è¿› Top50",
        "",
        "å½“å‘¨æ–°è¿›æ¦œå•ä¸”å½“å‰æ’ååœ¨ Top50 å†…çš„äº§å“ï¼ˆæŒ‰å½“å‰æ’åæ’åºï¼‰ï¼š",
        "",
        "| æ’å | äº§å“å | å¼€å‘è€… | å›½å®¶/åœ°åŒº | å¹³å° | ä¸‹è½½é‡ | æ”¶å…¥ |",
        "|------|--------|--------|-----------|------|--------|------|",
    ]
    for row in new_top50:
        lines.append(
            f"| {row['current_rank']} | {row['display_name']} | {row['publisher_name']} | {row['country']} | {row['platform']} | "
            f"{_fmt_num(row['downloads'])} | {_fmt_revenue(row['revenue'])} |"
        )
    if not new_top50:
        lines.append("| â€” | æœ¬å‘¨æ— æ–°è¿› Top50 è®°å½• | â€” | â€” | â€” | â€” | â€” |")
    lines.extend([
        "",
        "---",
        "",
        "## äºŒã€æœ¬å‘¨æ’åé£™å‡ Top10",
        "",
        "å½“å‘¨æ’åé£™å‡ä¸­ï¼Œä¸Šå‡å¹…åº¦æœ€å¤§çš„ 10 æ¬¾äº§å“ï¼š",
        "",
        "| å½“å‰æ’å | ä¸Šå‘¨æ’å | ä¸Šå‡å¹…åº¦ | äº§å“å | å¼€å‘è€… | å›½å®¶/åœ°åŒº | å¹³å° | ä¸‹è½½é‡ | æ”¶å…¥ |",
        "|----------|----------|----------|--------|--------|-----------|------|--------|------|",
    ])
    for row in surge_top10:
        lines.append(
            f"| {row['current_rank']} | {row['last_week_rank']} | {row['change']} | {row['display_name']} | {row['publisher_name']} | "
            f"{row['country']} | {row['platform']} | {_fmt_num(row['downloads'])} | {_fmt_revenue(row['revenue'])} |"
        )
    if not surge_top10:
        lines.append("| â€” | â€” | â€” | æœ¬å‘¨æ— æ’åé£™å‡è®°å½• | â€” | â€” | â€” | â€” | â€” |")
    lines.append("")
    lines.append(f"è¯¦æƒ…è¯·è¿›å…¥ [ç›‘æµ‹æ±‡æ€»å¹³å°]({DETAIL_LINK}) æŸ¥çœ‹ã€‚")
    return "\n".join(lines)


# ---------- å‘é€ ----------
def _post_json(url: str, payload: dict) -> tuple[int, str]:
    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        url,
        data=data,
        headers={"Content-Type": "application/json; charset=utf-8"},
        method="POST",
    )
    try:
        with urllib.request.urlopen(req, timeout=15) as resp:
            return resp.getcode(), resp.read().decode("utf-8", errors="ignore")
    except urllib.error.HTTPError as e:
        return e.code, e.read().decode("utf-8", errors="ignore")
    except urllib.error.URLError as e:
        return 0, str(e)


def send_feishu_card(webhook: str, title: str, md_content: str) -> None:
    """é£ä¹¦ï¼šå‘ä¸€æ¡äº’åŠ¨å¡ç‰‡ï¼Œæ ‡é¢˜ + Markdown æ­£æ–‡ã€‚"""
    payload = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": title},
                "template": "blue",
            },
            "elements": [{"tag": "markdown", "content": md_content}],
        },
    }
    status, resp = _post_json(webhook, payload)
    if status != 200:
        print(f"[é£ä¹¦] å‘é€å¤±è´¥ status={status} resp={resp}", file=sys.stderr)
    else:
        print("[é£ä¹¦] å‘é€æˆåŠŸ")


# ä¼ä¸šå¾®ä¿¡æœºå™¨äºº Markdown å•æ¡æ¶ˆæ¯ä¸Šé™ 4096 å­—èŠ‚ï¼ˆUTF-8ï¼‰ï¼Œè¶…é™ä¼šæŠ¥ 40058
WECOM_MARKDOWN_MAX_BYTES = 4096


def _truncate_for_wecom(md: str, max_bytes: int = WECOM_MARKDOWN_MAX_BYTES) -> str:
    """å°† Markdown æˆªæ–­åˆ°ä¸è¶…è¿‡ max_bytesï¼ˆUTF-8ï¼‰ï¼Œæœ«å°¾è¿½åŠ è¯¦è§é“¾æ¥ã€‚"""
    data = md.encode("utf-8")
    if len(data) <= max_bytes:
        return md
    suffix = f"\n\n> å†…å®¹è¿‡é•¿ï¼Œè¯¦è§ [ç›‘æµ‹æ±‡æ€»å¹³å°]({DETAIL_LINK}) æŸ¥çœ‹ã€‚"
    suffix_bytes = suffix.encode("utf-8")
    keep = max_bytes - len(suffix_bytes)
    if keep <= 0:
        return suffix.strip()
    # æŒ‰å­—èŠ‚æˆªæ–­ï¼Œé¿å…æˆªæ–­ UTF-8 å¤šå­—èŠ‚å­—ç¬¦ä¸­é—´
    chunk = data[:keep]
    while chunk and (chunk[-1] & 0x80) and not (chunk[-1] & 0x40):
        chunk = chunk[:-1]
    return chunk.decode("utf-8", errors="ignore") + suffix


def send_wecom_markdown(webhook: str, md_content: str) -> None:
    """ä¼ä¸šå¾®ä¿¡ï¼šå‘ä¸€æ¡ Markdown æ¶ˆæ¯ï¼ˆå•æ¡ä¸è¶…è¿‡ 4096 å­—èŠ‚ï¼‰ã€‚"""
    content = _truncate_for_wecom(md_content)
    payload = {
        "msgtype": "markdown",
        "markdown": {"content": content},
    }
    status, resp = _post_json(webhook, payload)
    if status != 200:
        print(f"[ä¼ä¸šå¾®ä¿¡] å‘é€å¤±è´¥ status={status} resp={resp}", file=sys.stderr)
    else:
        print("[ä¼ä¸šå¾®ä¿¡] å‘é€æˆåŠŸ")


def _clean_url(value: str | None) -> str | None:
    if not value:
        return None
    v = value.replace("\r", "").replace("\n", "").strip()
    if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
        v = v[1:-1].strip()
    return v if v else None


def main() -> int:
    parser = argparse.ArgumentParser(description="æ„å»ºä¸¤ä¸ªå°æ¸¸æˆå‘¨æŠ¥å¹¶å‘é€åˆ°é£ä¹¦ã€ä¼ä¸šå¾®ä¿¡")
    parser.add_argument(
        "--videos-db",
        type=Path,
        default=Path("public/videos.db"),
        help="videos.db è·¯å¾„ï¼ˆå¾®ä¿¡/æŠ–éŸ³å‘¨æŠ¥ï¼‰",
    )
    parser.add_argument(
        "--sensortower-db",
        type=Path,
        default=Path("public/sensortower_top100.db"),
        help="sensortower_top100.db è·¯å¾„ï¼ˆSensorTower å‘¨æŠ¥ï¼‰",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ„å»ºå†…å®¹å¹¶æ‰“å°ï¼Œä¸å‘é€",
    )
    args = parser.parse_args()

    repo_root = Path(__file__).resolve().parents[1]
    _load_env(repo_root)

    videos_db = repo_root / args.videos_db if not args.videos_db.is_absolute() else args.videos_db
    st_db = repo_root / args.sensortower_db if not args.sensortower_db.is_absolute() else args.sensortower_db

    parts: list[str] = []

    # 1) å¾®ä¿¡/æŠ–éŸ³å°æ¸¸æˆå‘¨æŠ¥
    if videos_db.exists():
        conn_wx = sqlite3.connect(str(videos_db))
        try:
            md_wx = build_wechat_douyin_weekly_md(conn_wx)
            if md_wx:
                parts.append(md_wx)
        finally:
            conn_wx.close()
    else:
        print(f"[è·³è¿‡] videos.db ä¸å­˜åœ¨: {videos_db}", file=sys.stderr)

    # 2) SensorTower å‘¨æŠ¥
    if st_db.exists():
        conn_st = sqlite3.connect(str(st_db))
        try:
            md_st = build_sensortower_weekly_md(conn_st)
            if md_st:
                parts.append(md_st)
        finally:
            conn_st.close()
    else:
        print(f"[è·³è¿‡] sensortower_top100.db ä¸å­˜åœ¨: {st_db}", file=sys.stderr)

    if not parts:
        print("æœªç”Ÿæˆä»»ä½•å‘¨æŠ¥å†…å®¹ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“ä¸è¡¨ç»“æ„ã€‚", file=sys.stderr)
        return 1

    # åˆå¹¶ä¸ºä¸€æ¡ï¼šä¸¤ä¸ªå‘¨æŠ¥ç”¨åˆ†éš”çº¿éš”å¼€
    combined_md = "\n\n---\n\n".join(parts)
    card_title = "å°æ¸¸æˆå‘¨æŠ¥ï¼ˆå¾®ä¿¡/æŠ–éŸ³ + SensorTowerï¼‰"

    if args.dry_run:
        print("=== æ„å»ºç»“æœï¼ˆdry-runï¼Œä¸å‘é€ï¼‰===")
        print(f"æ ‡é¢˜: {card_title}")
        print("---")
        print(combined_md)
        return 0

    feishu = _clean_url(os.environ.get("FEISHU_WEBHOOK_URL"))
    wecom = _clean_url(os.environ.get("WECOM_WEBHOOK_URL_REAL")) or _clean_url(os.environ.get("WECOM_WEBHOOK_URL"))
    if not feishu and not wecom:
        print(
            "æœªé…ç½® Webhookã€‚è¯·åœ¨ .env ä¸­è®¾ç½® FEISHU_WEBHOOK_URL æˆ– WECOM_WEBHOOK_URL_REALï¼ˆæˆ– WECOM_WEBHOOK_URLï¼‰",
            file=sys.stderr,
        )
        return 1

    if feishu:
        send_feishu_card(feishu, card_title, combined_md)
    if wecom:
        # ä¼ä¸šå¾®ä¿¡å•æ¡ Markdown é™åˆ¶ 4096 å­—èŠ‚ï¼Œåˆ†ä¸¤æ¡å‘é€ï¼šå¾®ä¿¡/æŠ–éŸ³ + SensorTower
        for i, part in enumerate(parts):
            send_wecom_markdown(wecom, part)

    return 0


if __name__ == "__main__":
    sys.exit(main())
